{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMjSSjgvg420Xtfq+gTGaEp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qsT-f6L0thZj"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Intsall packages\n","\n","# Drake\n","!pip install manipulation\n","# C-Extentions\n","!pip install cython  \n","# Pycocotools\n","!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n","\n","\n","# Clone TorchVision references/detection\n","!git clone https://github.com/pytorch/vision.git\n","!cd vision && git checkout v0.3.0\n","!cp vision/references/detection/utils.py ./\n","!cp vision/references/detection/transforms.py ./\n","!cp vision/references/detection/coco_eval.py ./\n","!cp vision/references/detection/engine.py ./\n","!cp vision/references/detection/coco_utils.py ./\n","\n","from manipulation import running_as_notebook\n","\n","# Import modules\n","import fnmatch\n","import json\n","import matplotlib.pyplot as plt\n","import multiprocessing\n","import numpy as np\n","import os\n","from PIL import Image\n","from IPython.display import display\n","\n","import torch\n","import torch.utils.data\n","\n","ycb = [\n","    \"003_cracker_box.sdf\", \"004_sugar_box.sdf\", \"005_tomato_soup_can.sdf\",\n","    \"006_mustard_bottle.sdf\", \"009_gelatin_box.sdf\", \"010_potted_meat_can.sdf\"\n","]\n","\n","#drake_reserved_labels = [32765, 32764, 32766, 32767]\n","\n","def colorize_labels(image):\n","    \"\"\"Colorizes labels.\"\"\"\n","    cc = mpl.colors.ColorConverter()\n","    color_cycle = plt.rcParams[\"axes.prop_cycle\"]\n","    colors = np.array([cc.to_rgb(c[\"color\"]) for c in color_cycle])\n","    bg_color = [0, 0, 0]\n","    image = np.squeeze(image)\n","    background = np.zeros(image.shape[:2], dtype=bool)\n","    for label in reserved_labels:\n","        background |= image == int(label)\n","    foreground = image[np.logical_not(background)]\n","    color_image = colors[image % len(colors)]\n","    color_image[background] = bg_color\n","    return color_image\n"],"metadata":{"id":"5aiP5RZM9_T4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Unzip dataset\n","dataset_path = \"clutter_maskrcnn_data\"\n","if not os.path.exists(dataset_path):\n","  !cp /content/drive/MyDrive/clutter_maskrcnn_data.zip ./\n","  !unzip -q clutter_maskrcnn_data.zip"],"metadata":{"id":"7xcjAwDEG4dy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset class definition\n","class BinPickingDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms=None):\n","        self.root = root\n","        self.num_images = len(fnmatch.filter(os.listdir(root),'*.png'))\n","        self.transforms = transforms\n","\n","    def __getitem__(self, idx):\n","        filename_base = os.path.join(self.root, f\"{idx:05d}\")\n","\n","        img = Image.open(filename_base + \".png\").convert(\"RGB\")\n","        mask = np.squeeze(np.load(filename_base + \"_mask.npy\"))\n","\n","        with open(filename_base + \".json\", \"r\") as f:\n","            instance_id_to_class_name = json.load(f)\n","        labels = ycb == instance_id_to_class_name\n","\n","        # instances are encoded as different colors\n","        obj_ids = np.asarray(list(instance_id_to_class_name.keys()))\n","        count = (mask == np.int16(obj_ids)[:, None, None]).sum(axis=2).sum(axis=1)\n","        \n","        # discard objects instances with less than 10 pixels\n","        obj_ids = obj_ids[count >= 10]\n","\n","        labels = [ycb.index(instance_id_to_class_name[id]+\".sdf\") for id in obj_ids]\n","        obj_ids = np.int16(np.asarray(obj_ids))\n","\n","        # split the color-encoded mask into a set of binary masks\n","        masks = mask == obj_ids[:, None, None]\n","\n","        # get bounding box coordinates for each mask\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.where(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return self.num_images\n"],"metadata":{"id":"NXOcPuLtH4PE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = BinPickingDataset(dataset_path)\n","dataset[0][0]"],"metadata":{"id":"3NNU8B0_KJwf"},"execution_count":null,"outputs":[]}]}